ğŸš• NYC Taxi Data Engineering Pipeline
ğŸ“Œ Project Overview

This project implements an end-to-end Data Engineering ETL pipeline using the NYC Yellow Taxi Trip dataset to demonstrate real-world data ingestion, transformation, data modeling, and analytics.

The pipeline processes 3.4+ million taxi trip records, applies data quality checks, designs an OLAP star schema, loads the data into MySQL, and supports analytical querying and dashboarding.

ğŸ—ï¸ Architecture
NYC Taxi Parquet Files
        â†“
Python ETL (Pandas, PyArrow)
        â†“
Data Cleaning & Transformation
        â†“
Star Schema (Fact & Dimensions)
        â†“
MySQL (OLAP Database)
        â†“
Analytics SQL Queries

ğŸ“Š Dataset

Source: NYC Yellow Taxi Trip Data

Format: Parquet

Volume: ~3.4 million records (January 2025)

Key Attributes: pickup time, dropoff time, trip distance, fare amount, payment type, locations

ğŸ§  Data Modeling (Star Schema)
â­ Fact Table

fact_nyc_taxi_data

trip_distance

fare_amount

tip_amount

total_amount

passenger_count

trip_duration_sec

date_id (FK)

payment_id (FK)

pickup_location_id

dropoff_location_id

â­ Dimension Tables

dim_date â€“ date, day, month, year, weekday/weekend

dim_location â€“ pickup & dropoff location IDs

dim_payment â€“ payment types

This structure enables fast, scalable analytical queries.

ğŸ” ETL Pipeline Steps
1ï¸âƒ£ Extract

Read large Parquet files using PyArrow

Convert to Pandas DataFrame

Persist raw data for traceability

2ï¸âƒ£ Transform

Type casting with errors='coerce'

Data quality filtering (invalid fares, distances, timestamps)

Feature engineering (trip duration, date attributes)

Column standardization

3ï¸âƒ£ Load

Chunk-based bulk inserts using SQLAlchemy

Load dimension tables first

Map surrogate keys (date_id)

Load fact table

Supports safe reprocessing

âš™ï¸ Technologies Used

Python

Pandas

PyArrow

SQLAlchemy

MySQL

SQL

Git & GitHub

ğŸ“ˆ Analytics Use Cases

Daily and monthly revenue trends

Weekday vs weekend trip analysis

Payment method distribution

High-demand pickup/dropoff locations

Average trip distance and fare analysis

ğŸš€ How to Run the Project
1ï¸âƒ£ Clone Repository
git clone https://github.com/JaiKrishna1065/NYC_Taxi_Trip_Data_PipeLine.git
cd NYC_Taxi_Trip_Data_PipeLine

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Configure Database

Create db_config.py using sample values:

DB_URL = "mysql+mysqlconnector://username:password@localhost:3306/nyc_taxi"

4ï¸âƒ£ Run ETL Pipeline
python main.py

ğŸ“ Project Structure
NYC_Taxi_Trip_Data_PipeLine/
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ db_config_sample.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql
â”‚   â”œâ”€â”€ analytics_queries.sql
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/        (ignored)
â”‚   â””â”€â”€ processed/ (ignored)
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ğŸ›¡ï¸ Best Practices Implemented

Chunk-based processing for large datasets

Logging for pipeline monitoring

Star schema modeling for OLAP workloads

Idempotent and restart-safe design

Clean Git repository management

ğŸ”® Future Enhancements

Automate pipeline using Apache Airflow

Migrate data warehouse to Snowflake

Implement incremental data loading

Add data quality validation framework

Deploy dashboards to cloud BI tools
